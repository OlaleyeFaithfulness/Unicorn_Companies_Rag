{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwL7ciuerSTRGwgcFDtyqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlaleyeFaithfulness/Unicorn_Companies_Rag/blob/main/unicorn_rag_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuYwJUTu9kFp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# -------------------------------------------------\n",
        "# MODEL CONFIG\n",
        "# -------------------------------------------------\n",
        "\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding model**"
      ],
      "metadata": {
        "id": "hTge2XddkTtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up Embedding Model & Imports\n",
        "\n",
        "# Imports\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document  # LangChain core Document class\n",
        "from langchain_community.vectorstores.faiss import FAISS as LCFAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# ----------------------------\n",
        "# 1. Load BGE embedding model\n",
        "# ----------------------------\n",
        "embedding_model = SentenceTransformerEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en\",\n",
        "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
        ")\n",
        "\n",
        "print(\"BAAI/bge-large-en embedding model loaded. Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "CBDpNIVjkUIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and clean the csv**"
      ],
      "metadata": {
        "id": "3fzDOhv1nkKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Load Dataset\n",
        "# ----------------------------\n",
        "\n",
        "DATA_PATH = \"datasets/unicorn_companies.csv\"\n",
        "\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "\n",
        "print(f\"Dataset loaded successfully.\")\n",
        "print(f\"DF Shape: {df.shape}\")\n",
        "print(f\"DF Columns: {list(df.columns)}\")\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "WGWCPceBnkgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Cleaning Dataset\n",
        "# ----------------------------\n",
        "\n",
        "# Standardize column names\n",
        "df.columns = (\n",
        "    df.columns\n",
        "      .str.strip()                 # remove whitespace\n",
        "      .str.lower()                 # lowercase\n",
        "      .str.replace(\" \", \"_\", regex=True)  # spaces -> underscores\n",
        "      .str.replace(\"(\", \"\", regex=True)\n",
        "      .str.replace(\")\", \"\", regex=True)\n",
        "      .str.replace(\"$\", \"\", regex=True)\n",
        ")\n",
        "\n",
        "# Rename specific columns for clarity\n",
        "df.rename(columns={\"valuation_b\": \"valuation\"}, inplace=True)\n",
        "\n",
        "# Quick sanity checks\n",
        "print(\"Columns after cleaning:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nDataFrame info:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nRows with missing 'select_investors':\")\n",
        "print(df[df[\"select_investors\"].isna()])\n"
      ],
      "metadata": {
        "id": "6A7diNpxobyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating langchain docs and metadata**"
      ],
      "metadata": {
        "id": "jxtnv0papCNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Create LangChain Documents with Metadata\n",
        "# ----------------------------\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "lc_docs = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    # Handle missing investors\n",
        "    investors = row[\"select_investors\"]\n",
        "    investors_text = (\n",
        "        f\"Select investors: {investors}\" if pd.notna(investors)\n",
        "        else \"Select investors not publicly listed\"\n",
        "    )\n",
        "\n",
        "    # Format date safely\n",
        "    try:\n",
        "        pretty_date = row[\"date_added\"].strftime(\"%B %d, %Y\")\n",
        "    except Exception:\n",
        "        pretty_date = \"Date not available\"\n",
        "\n",
        "    # Build the text to embed\n",
        "    page_content = (\n",
        "        f\"Company: {row['company']}\\n\"\n",
        "        f\"Category: {row['category']}\\n\"\n",
        "        f\"Country: {row['country']}\\n\"\n",
        "        f\"Valuation: ${row['valuation']}B\\n\"\n",
        "        f\"Date added to unicorn list: {pretty_date}\\n\"\n",
        "        f\"{investors_text}\"\n",
        "    )\n",
        "\n",
        "    # Metadata for retrieval\n",
        "    metadata = {\n",
        "        \"row_id\": int(i),\n",
        "        \"company\": row[\"company\"],\n",
        "        \"country\": row[\"country\"],\n",
        "        \"category\": row[\"category\"],\n",
        "        \"valuation\": row[\"valuation\"],\n",
        "        \"date_added\": row[\"date_added\"]  # raw datetime\n",
        "    }\n",
        "\n",
        "    lc_docs.append(Document(page_content=page_content, metadata=metadata))\n",
        "\n",
        "print(f\"Created {len(lc_docs)} Document objects.\")\n",
        "\n",
        "# Optional: inspect some entries\n",
        "print(\"\\nSample page content:\")\n",
        "print(lc_docs[0].page_content)\n",
        "\n",
        "print(\"\\nSample metadata:\")\n",
        "print(lc_docs[0].metadata)\n"
      ],
      "metadata": {
        "id": "_0uuaryIpCs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building vectorestore**"
      ],
      "metadata": {
        "id": "LWzqtNZ6pc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Embed Documents into FAISS Vector Store\n",
        "# ----------------------------\n",
        "\n",
        "# Build the vector store\n",
        "vectorstore = LCFAISS.from_documents(\n",
        "    documents=lc_docs,\n",
        "    embedding=embedding_model,\n",
        "    normalize_L2=True\n",
        ")\n",
        "\n",
        "print(f\"Vector store built successfully. Documents indexed: {len(lc_docs)}\")\n"
      ],
      "metadata": {
        "id": "OxB_4nOjpdQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build retriever**"
      ],
      "metadata": {
        "id": "W7NSG49sqG2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Retriever Setup\n",
        "# ----------------------------\n",
        "\n",
        "# Standard retriever (top 5 similar docs)\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}  # tune number of results\n",
        ")\n",
        "\n",
        "print(\"Retriever ready. Returns top 5 similar documents.\")\n",
        "\n",
        "# Optional: metadata-filtered retriever example\n",
        "# Uncomment to use\n",
        "# category_retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"similarity\",\n",
        "#     search_kwargs={\"k\": 4},\n",
        "#     filter={\"category\": \"Artificial intelligence\"}  # filters docs BEFORE search\n",
        "# )\n",
        "# print(\"Category-filtered retriever ready for AI category.\")\n"
      ],
      "metadata": {
        "id": "V8GQ1H6jqHOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check similarity scores**"
      ],
      "metadata": {
        "id": "q_CEfl6oqmA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Query Vector Store with Similarity Scores\n",
        "# ----------------------------\n",
        "\n",
        "query = \"tell me about companies that achieved unicorn status in 2021\"\n",
        "\n",
        "# Perform similarity search\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=4)\n",
        "\n",
        "print(f\"Top {len(results_with_scores)} results for query: '{query}'\\n\")\n",
        "\n",
        "for i, (doc, score) in enumerate(results_with_scores, start=1):\n",
        "    print(f\"Result {i}\")\n",
        "    print(f\"Similarity score: {score:.4f}\")  # FAISS score (depends on metric)\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print(f\"Text preview:\\n{doc.page_content[:300]}{'...' if len(doc.page_content) > 300 else ''}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "ehICdzf8qmYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build the prompt**"
      ],
      "metadata": {
        "id": "zONbiTEErTge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Build Prompt Template\n",
        "# ----------------------------\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "PROMPT = PromptTemplate.from_template(\"\"\"\n",
        "You are CHAD, CHAD stands for (Computational Hyper-Advanced Decoder), a personal AI created by Olaleye Faithfulness Ibukun.\n",
        "\n",
        "You know about unicorn companies and their info ONLY from the context provided below.\n",
        "\n",
        "------------------\n",
        "CONTEXT:\n",
        "{context}\n",
        "------------------\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\")\n",
        "\n",
        "print(\"Prompt template created successfully.\")\n"
      ],
      "metadata": {
        "id": "FiGPjPqUrWpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LcApaW4HryEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Helper Function: Format Docs into Context\n",
        "# ----------------------------\n",
        "def format_docs(docs):\n",
        "    \"\"\"Combine retrieved Document objects into a single string context.\"\"\"\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Ask CHAD: RAG + LLM Chain\n",
        "# ----------------------------\n",
        "def ask_chad(question):\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents, build context, and generate an answer using CHAD.\n",
        "\n",
        "    Args:\n",
        "        question (str): User query\n",
        "\n",
        "    Returns:\n",
        "        str: Answer generated by CHAD\n",
        "    \"\"\"\n",
        "    # Step A — Retrieve documents\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    # Step B — Build context string\n",
        "    context = format_docs(docs) if docs else \"NO RELEVANT CONTEXT FOUND.\"\n",
        "\n",
        "    # Step C — Fill prompt template\n",
        "    prompt = PROMPT.format(\n",
        "        context=context,\n",
        "        question=question\n",
        "    )\n",
        "\n",
        "    # Step D — Tokenize for Qwen2.5\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Step E — Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_p=0.6,\n",
        "            eos_token_id=tokenizer.eos_token_id  # stop at end-of-sequence\n",
        "        )\n",
        "\n",
        "    # Step F — Decode output\n",
        "    answer = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Step G — Remove prompt echo if present\n",
        "    answer_only = answer.split(\"ANSWER:\")[-1].strip()\n",
        "\n",
        "    return answer_only\n"
      ],
      "metadata": {
        "id": "OYYAQa7KrydR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the AI**"
      ],
      "metadata": {
        "id": "vg5KRw55sQLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which companies reached unicorn status in 2021?\"\n",
        "answer = ask_chad(question)\n",
        "print(\"CHAD says:\\n\", answer)\n"
      ],
      "metadata": {
        "id": "5VC_OxOrsVXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}